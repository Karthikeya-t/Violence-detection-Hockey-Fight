{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{"id":"UVXa7uMnOlkp"}},{"cell_type":"code","source":"%matplotlib inline\nimport cv2\nimport os\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\n# import download\nfrom random import shuffle\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Activation\nimport sys\nimport h5py","metadata":{"id":"oDfDnlliPMd-","outputId":"7582c7ed-2ba2-4164-e043-d0c8d31ba19f","execution":{"iopub.status.busy":"2022-01-21T17:18:42.719679Z","iopub.execute_input":"2022-01-21T17:18:42.720018Z","iopub.status.idle":"2022-01-21T17:18:47.425317Z","shell.execute_reply.started":"2022-01-21T17:18:42.719944Z","shell.execute_reply":"2022-01-21T17:18:47.424507Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"keras.__version__","metadata":{"id":"kFU6Qg8mPXvv","outputId":"04a2d803-e425-4335-94e8-71e91d05d6b1","execution":{"iopub.status.busy":"2022-01-21T17:18:47.428560Z","iopub.execute_input":"2022-01-21T17:18:47.428837Z","iopub.status.idle":"2022-01-21T17:18:47.437419Z","shell.execute_reply.started":"2022-01-21T17:18:47.428809Z","shell.execute_reply":"2022-01-21T17:18:47.436304Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"v6pf1l28PzIO"}},{"cell_type":"markdown","source":"We will use the function ```print_progress``` to print the amount of videos processed the datasets","metadata":{"id":"WKnyJkf8PzxE"}},{"cell_type":"code","source":"def print_progress(count, max_count):\n    # Percentage completion.\n    pct_complete = count / max_count\n\n    # Status-message. Note the \\r which means the line should\n    # overwrite itself.\n    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n\n    # Print it.\n    sys.stdout.write(msg)\n    sys.stdout.flush()","metadata":{"id":"qnafWmS7P3CG","execution":{"iopub.status.busy":"2022-01-21T17:18:47.439083Z","iopub.execute_input":"2022-01-21T17:18:47.439693Z","iopub.status.idle":"2022-01-21T17:18:47.444989Z","shell.execute_reply.started":"2022-01-21T17:18:47.439653Z","shell.execute_reply":"2022-01-21T17:18:47.444162Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n\nFirstly, we define the directory to place the video dataset","metadata":{"id":"tRf-KgkjP9Kt"}},{"cell_type":"code","source":"in_dir = \"../input/hockey-fight-vidoes/data\"","metadata":{"id":"RiRKgwBgP-NY","execution":{"iopub.status.busy":"2022-01-21T17:18:47.446607Z","iopub.execute_input":"2022-01-21T17:18:47.447243Z","iopub.status.idle":"2022-01-21T17:18:47.454284Z","shell.execute_reply.started":"2022-01-21T17:18:47.447193Z","shell.execute_reply":"2022-01-21T17:18:47.453337Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Copy some of the data-dimensions for convenience.","metadata":{"id":"UCZHrpJjRKky"}},{"cell_type":"code","source":"# Frame size  \nimg_size = 256\n\nimg_size_touple = (img_size, img_size)\n\n# Number of channels (RGB)\nnum_channels = 3\n\n# Flat frame size\nimg_size_flat = img_size * img_size * num_channels\n\n# Number of classes for classification (Violence-No Violence)\nnum_classes = 2\n\n# Number of files to train\n_num_files_train = 1\n\n# Number of frames per video\n_images_per_file = 20\n\n# Number of frames per training set\n_num_images_train = _num_files_train * _images_per_file\n\n# Video extension\nvideo_exts = \".avi\"","metadata":{"id":"SXTNEj6SRLZZ","execution":{"iopub.status.busy":"2022-01-21T17:18:47.458477Z","iopub.execute_input":"2022-01-21T17:18:47.458795Z","iopub.status.idle":"2022-01-21T17:18:47.464125Z","shell.execute_reply.started":"2022-01-21T17:18:47.458770Z","shell.execute_reply":"2022-01-21T17:18:47.463137Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_frames(current_dir, file_name):\n    \n    in_file = os.path.join(current_dir, file_name)\n    \n    images = []\n    \n    vidcap = cv2.VideoCapture(in_file)\n    \n    success,image = vidcap.read()\n        \n    count = 0\n\n    while count<_images_per_file:\n                \n        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n                                 interpolation=cv2.INTER_CUBIC)\n    \n        images.append(res)\n    \n        success,image = vidcap.read()\n    \n        count += 1\n        \n    resul = np.array(images)\n    \n    resul = (resul / 255.).astype(np.float16)\n        \n    return resul","metadata":{"id":"eu9c4a-3RVkO","execution":{"iopub.status.busy":"2022-01-21T17:18:47.467628Z","iopub.execute_input":"2022-01-21T17:18:47.468365Z","iopub.status.idle":"2022-01-21T17:18:47.475756Z","shell.execute_reply.started":"2022-01-21T17:18:47.468329Z","shell.execute_reply":"2022-01-21T17:18:47.474961Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Helper function to get the names of the data downloaded and label it","metadata":{"id":"tLCjYFBtRZb-"}},{"cell_type":"code","source":"def label_video_names(in_dir):\n    \n    # list containing video names\n    names = []\n    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n    labels = []\n    \n    \n    for current_dir, dir_names,file_names in os.walk(in_dir):\n        \n        for file_name in file_names:\n            \n            if file_name[0:2] == 'fi':\n                labels.append([1,0])\n                names.append(file_name)\n            elif file_name[0:2] == 'no':\n                labels.append([0,1])\n                names.append(file_name)\n                     \n            \n    c = list(zip(names,labels))\n    # Suffle the data (names and labels)\n    shuffle(c)\n    \n    names, labels = zip(*c)\n            \n    return names, labels","metadata":{"id":"Qiv5NIJjRbIA","execution":{"iopub.status.busy":"2022-01-21T17:18:47.477393Z","iopub.execute_input":"2022-01-21T17:18:47.478002Z","iopub.status.idle":"2022-01-21T17:18:47.486577Z","shell.execute_reply.started":"2022-01-21T17:18:47.477965Z","shell.execute_reply":"2022-01-21T17:18:47.485727Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Plot a video frame to see if data is correct","metadata":{"id":"t3KW2kfgReKn"}},{"cell_type":"code","source":"# First get the names and labels of the whole videos\nnames, labels = label_video_names(in_dir)","metadata":{"id":"dIsaAgcyRfIx","execution":{"iopub.status.busy":"2022-01-21T17:18:47.493274Z","iopub.execute_input":"2022-01-21T17:18:47.493874Z","iopub.status.idle":"2022-01-21T17:18:47.935156Z","shell.execute_reply.started":"2022-01-21T17:18:47.493835Z","shell.execute_reply":"2022-01-21T17:18:47.934251Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Then we are going to load 20 frames of one video, for example","metadata":{"id":"EqucOMsJRgqm"}},{"cell_type":"code","source":"names[12]","metadata":{"id":"xUfZO-0BRj0f","outputId":"e2b913dc-c39f-4d5f-a0d6-da1273692fbc","execution":{"iopub.status.busy":"2022-01-21T17:18:47.939047Z","iopub.execute_input":"2022-01-21T17:18:47.939395Z","iopub.status.idle":"2022-01-21T17:18:47.946916Z","shell.execute_reply.started":"2022-01-21T17:18:47.939366Z","shell.execute_reply":"2022-01-21T17:18:47.945686Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The video has violence, look at the name of the video, starts with 'fi'","metadata":{"id":"ORGJ2pS9RnWw"}},{"cell_type":"code","source":"frames = get_frames(in_dir, names[12])","metadata":{"id":"EqBi8z6rRoMW","execution":{"iopub.status.busy":"2022-01-21T17:18:47.948754Z","iopub.execute_input":"2022-01-21T17:18:47.949586Z","iopub.status.idle":"2022-01-21T17:18:48.116093Z","shell.execute_reply.started":"2022-01-21T17:18:47.949532Z","shell.execute_reply":"2022-01-21T17:18:48.115303Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Convert back the frames to uint8 pixel format to plot the frame","metadata":{"id":"vtgQEmI6RrmM"}},{"cell_type":"code","source":"visible_frame = (frames*255).astype('uint8')","metadata":{"id":"9ihSA_ogRsNU","execution":{"iopub.status.busy":"2022-01-21T17:18:48.119426Z","iopub.execute_input":"2022-01-21T17:18:48.119678Z","iopub.status.idle":"2022-01-21T17:18:48.181393Z","shell.execute_reply.started":"2022-01-21T17:18:48.119652Z","shell.execute_reply":"2022-01-21T17:18:48.180718Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.imshow(visible_frame[3])","metadata":{"id":"PM1kNhaHRvSv","outputId":"e2a6d324-cb62-42d2-dd8f-1cf1c464d168","execution":{"iopub.status.busy":"2022-01-21T17:18:48.182544Z","iopub.execute_input":"2022-01-21T17:18:48.182872Z","iopub.status.idle":"2022-01-21T17:18:48.343209Z","shell.execute_reply.started":"2022-01-21T17:18:48.182846Z","shell.execute_reply":"2022-01-21T17:18:48.342396Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plt.imshow(visible_frame[15])","metadata":{"id":"_gVWtYPvR8n2","outputId":"f9858ffa-3981-4b85-9c4d-62057bb8972c","execution":{"iopub.status.busy":"2022-01-21T17:18:48.344720Z","iopub.execute_input":"2022-01-21T17:18:48.345052Z","iopub.status.idle":"2022-01-21T17:18:48.488430Z","shell.execute_reply.started":"2022-01-21T17:18:48.345015Z","shell.execute_reply":"2022-01-21T17:18:48.486910Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_addons","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:48.490412Z","iopub.execute_input":"2022-01-21T17:18:48.490895Z","iopub.status.idle":"2022-01-21T17:18:55.137954Z","shell.execute_reply.started":"2022-01-21T17:18:48.490864Z","shell.execute_reply":"2022-01-21T17:18:55.137107Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom keras.applications import imagenet_utils\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n\nimport tensorflow_datasets as tfds\nimport tensorflow_addons as tfa\n\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.141230Z","iopub.execute_input":"2022-01-21T17:18:55.141512Z","iopub.status.idle":"2022-01-21T17:18:55.664176Z","shell.execute_reply.started":"2022-01-21T17:18:55.141484Z","shell.execute_reply":"2022-01-21T17:18:55.663460Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"patch_size = 4\nimage_size = 256\nimage_size_large = 270\nexpansion_ratio = 2.0\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.667353Z","iopub.execute_input":"2022-01-21T17:18:55.667600Z","iopub.status.idle":"2022-01-21T17:18:55.673163Z","shell.execute_reply.started":"2022-01-21T17:18:55.667575Z","shell.execute_reply":"2022-01-21T17:18:55.672501Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class InvertedRes(layers.Layer):\n  def __init__(self, expand_channels, output_channels, strides=1):\n    super().__init__()\n    self.output_channels = output_channels\n    self.strides = strides\n    self.expand = keras.models.Sequential([\n                                          layers.Conv2D(expand_channels, 1, padding=\"same\", use_bias=False),\n                                          layers.BatchNormalization(),\n                                          layers.Activation('swish')\n                                        ], name=\"expand\")\n    self.dw_conv = keras.models.Sequential([\n                                          layers.DepthwiseConv2D(3, strides=strides, padding=\"same\", use_bias=False),\n                                          layers.BatchNormalization(),\n                                          layers.Activation('swish')\n                                        ], name=\"depthwise\")\n    self.pw_conv = keras.models.Sequential([\n                                          layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False),\n                                          layers.BatchNormalization(),\n                                        ], name='pointwise')\n  \n  def call(self, x):\n    o = self.expand(x)\n    o = self.dw_conv(o)\n    o = self.pw_conv(o)\n    if self.strides == 1 and o.shape[-1] == self.output_channels:\n      return o + x\n    return o","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.676370Z","iopub.execute_input":"2022-01-21T17:18:55.676766Z","iopub.status.idle":"2022-01-21T17:18:55.688099Z","shell.execute_reply.started":"2022-01-21T17:18:55.676737Z","shell.execute_reply":"2022-01-21T17:18:55.687348Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class FullyConnected(layers.Layer):\n  def __init__(self, hidden_units, dropout_rate):\n    super().__init__()\n    l = []\n    for units in hidden_units:\n      l.append(layers.Dense(units, activation=tf.nn.swish))\n      l.append(layers.Dropout(dropout_rate))\n    self.mlp = keras.models.Sequential(l)\n\n  def call(self, x):\n    return self.mlp(x)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.691355Z","iopub.execute_input":"2022-01-21T17:18:55.691616Z","iopub.status.idle":"2022-01-21T17:18:55.699172Z","shell.execute_reply.started":"2022-01-21T17:18:55.691574Z","shell.execute_reply":"2022-01-21T17:18:55.698366Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Transformer(layers.Layer):\n  def __init__(self, projection_dim, heads=2):\n    super().__init__()\n    self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n    self.attention = layers.MultiHeadAttention(num_heads=heads, key_dim=projection_dim, dropout=0.1)\n    self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n  \n  def build(self, input_shape):\n    self.mlp = FullyConnected([input_shape[-1] * 2, input_shape[-1]], dropout_rate=0.1)\n\n\n  def call(self, x):\n    x1 = self.norm1(x)\n    att = self.attention(x1, x1)\n    x2 = x + att\n    x3 = self.norm2(x2)\n    x3 = self.mlp(x3)\n    return x3 + x2","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.700970Z","iopub.execute_input":"2022-01-21T17:18:55.701644Z","iopub.status.idle":"2022-01-21T17:18:55.710357Z","shell.execute_reply.started":"2022-01-21T17:18:55.701604Z","shell.execute_reply":"2022-01-21T17:18:55.709428Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class MobileVitBlock(layers.Layer):\n  def __init__(self, num_blocks, projection_dim, strides=1):\n    super().__init__()\n    self.projection_dim = projection_dim\n    self.conv_local = keras.models.Sequential([\n                                           layers.Conv2D(projection_dim, 3, padding=\"same\", strides=strides, activation=tf.nn.swish),\n                                           layers.Conv2D(projection_dim, 1, padding=\"same\", strides=strides, activation=tf.nn.swish),\n                                           ])\n    self.transformers = keras.models.Sequential([Transformer(projection_dim, heads=2) for i in range(num_blocks)])\n    self.conv_folded = layers.Conv2D(projection_dim, 1, padding=\"same\", strides=strides, activation=tf.nn.swish)\n    self.conv_local_global = layers.Conv2D(projection_dim, 3, padding=\"same\", strides=strides, activation=tf.nn.swish)\n\n  def build(self, input_shape):\n    num_patches = int((input_shape[1] * input_shape[2]) / patch_size)\n    self.unfold = layers.Reshape((patch_size, num_patches, self.projection_dim))\n    self.fold = layers.Reshape((input_shape[1], input_shape[2], self.projection_dim))\n\n  def call(self, x):\n    local_features = self.conv_local(x)\n    patches = self.unfold(local_features)\n    global_features = self.transformers(patches)\n    folded_features = self.fold(global_features)\n    folded_features = self.conv_folded(folded_features)\n    local_global_features = tf.concat([x, folded_features], axis=-1)\n    local_global_features = self.conv_local_global(local_global_features)\n    return local_global_features","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.711822Z","iopub.execute_input":"2022-01-21T17:18:55.712243Z","iopub.status.idle":"2022-01-21T17:18:55.724958Z","shell.execute_reply.started":"2022-01-21T17:18:55.712137Z","shell.execute_reply":"2022-01-21T17:18:55.724124Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class MobileVit(keras.models.Model):\n  def __init__(self):\n    super().__init__()\n    self.features = keras.models.Sequential([ \n                                              layers.Conv2D(16, 1, padding=\"same\", strides=(2, 2), activation=tf.nn.swish),\n                                              InvertedRes(16 * expansion_ratio, 16, strides=1),\n                                              InvertedRes(16 * expansion_ratio, 24, strides=2),\n                                              InvertedRes(24 * expansion_ratio, 24, strides=1),\n                                              InvertedRes(24 * expansion_ratio, 24, strides=1),\n                                              InvertedRes(24 * expansion_ratio, 48, strides=2),\n                                              MobileVitBlock(2, 64, strides=1),\n                                              layers.Conv2D(1, 1, padding=\"same\", strides=(1, 1), activation=tf.nn.swish)\n                                            ], name = \"features\")\n    \n  \n    \n  def call(self, x):\n    features = self.features(x)\n    return features","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.726496Z","iopub.execute_input":"2022-01-21T17:18:55.726993Z","iopub.status.idle":"2022-01-21T17:18:55.737747Z","shell.execute_reply.started":"2022-01-21T17:18:55.726956Z","shell.execute_reply":"2022-01-21T17:18:55.736988Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = MobileVit()\nmodel.build((None, 256, 256, 1))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:55.739470Z","iopub.execute_input":"2022-01-21T17:18:55.739973Z","iopub.status.idle":"2022-01-21T17:18:59.478626Z","shell.execute_reply.started":"2022-01-21T17:18:55.739934Z","shell.execute_reply":"2022-01-21T17:18:59.477846Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:59.479934Z","iopub.execute_input":"2022-01-21T17:18:59.480287Z","iopub.status.idle":"2022-01-21T17:18:59.499855Z","shell.execute_reply.started":"2022-01-21T17:18:59.480253Z","shell.execute_reply":"2022-01-21T17:18:59.498839Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"[original paper](https://arxiv.org/abs/2110.02178)):\n\n\n![](https://i.imgur.com/mANnhI7.png)","metadata":{}},{"cell_type":"markdown","source":"![](https://i.ibb.co/sRbVRBN/image.png)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileVit=Sequential()\nMobileVit.add(layers.InputLayer(input_shape=(256, 256, 3)))\nMobileVit.add(layers.Conv2D(16, 3, padding=\"same\", strides=(2, 2), activation=tf.nn.swish))\nMobileVit.add(InvertedRes(16 * expansion_ratio, 16, strides=1))\nMobileVit.add(InvertedRes(16 * expansion_ratio, 24, strides=2))\nMobileVit.add(InvertedRes(24 * expansion_ratio, 24, strides=1))\nMobileVit.add(InvertedRes(24 * expansion_ratio, 24, strides=1))\nMobileVit.add(InvertedRes(24 * expansion_ratio, 48, strides=2))\nMobileVit.add(MobileVitBlock(2, 64, strides=1))\nMobileVit.add(InvertedRes(64 * expansion_ratio, 64, strides=2))\nMobileVit.add(MobileVitBlock(4, 80, strides=1))\nMobileVit.add(InvertedRes(80 * expansion_ratio, 80, strides=2))\nMobileVit.add(MobileVitBlock(3, 96, strides=1))\nMobileVit.add(layers.Conv2D(64, 1, padding=\"same\", strides=(1, 1), activation=tf.nn.swish))\nMobileVit.add(layers.Flatten())\nMobileVit.build((None, 256, 256, 1))\nMobileVit.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:18:59.501902Z","iopub.execute_input":"2022-01-21T17:18:59.502319Z","iopub.status.idle":"2022-01-21T17:19:02.068075Z","shell.execute_reply.started":"2022-01-21T17:18:59.502275Z","shell.execute_reply":"2022-01-21T17:19:02.067351Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"image_model =MobileVit","metadata":{"id":"MjRN6oE4SC81","outputId":"1a9fbd16-9741-406d-b4fd-e3a20cd34a0d","execution":{"iopub.status.busy":"2022-01-21T17:19:02.072567Z","iopub.execute_input":"2022-01-21T17:19:02.072896Z","iopub.status.idle":"2022-01-21T17:19:02.077653Z","shell.execute_reply.started":"2022-01-21T17:19:02.072868Z","shell.execute_reply":"2022-01-21T17:19:02.076690Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"image_model.summary()\n","metadata":{"id":"ud7OU0t7SQPi","outputId":"20718d3a-c1d3-4a06-d6f5-de3c465311df","execution":{"iopub.status.busy":"2022-01-21T17:19:02.079098Z","iopub.execute_input":"2022-01-21T17:19:02.079470Z","iopub.status.idle":"2022-01-21T17:19:02.109321Z","shell.execute_reply.started":"2022-01-21T17:19:02.079431Z","shell.execute_reply":"2022-01-21T17:19:02.108599Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\ntransfer_layer = image_model.get_layer('flatten')\nimage_model_transfer = Model(inputs=image_model.input,\n                             outputs=transfer_layer.output)\n\n\n\n\ntransfer_values_size = K.int_shape(transfer_layer.output)[1]\n\n\nprint(\"The input of the VGG16 net have dimensions:\",K.int_shape(image_model.input)[1:3])\n\nprint(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)","metadata":{"id":"4YWFA-2tSdfB","outputId":"55dcb0c7-2b8a-4420-e25e-381152197a81","execution":{"iopub.status.busy":"2022-01-21T17:19:02.112710Z","iopub.execute_input":"2022-01-21T17:19:02.112945Z","iopub.status.idle":"2022-01-21T17:19:02.123804Z","shell.execute_reply.started":"2022-01-21T17:19:02.112922Z","shell.execute_reply":"2022-01-21T17:19:02.122701Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def get_transfer_values(current_dir, file_name):\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n    image_batch = get_frames(current_dir, file_name)\n      \n    # Pre-allocate output-array for transfer-values.\n    # Note that we use 16-bit floating-points to save memory.\n    shape = (_images_per_file, transfer_values_size)\n    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n\n    transfer_values = \\\n            image_model_transfer.predict(image_batch)\n            \n    return transfer_values","metadata":{"id":"-AdxYAtiSlLF","execution":{"iopub.status.busy":"2022-01-21T17:19:02.125674Z","iopub.execute_input":"2022-01-21T17:19:02.125997Z","iopub.status.idle":"2022-01-21T17:19:02.132887Z","shell.execute_reply.started":"2022-01-21T17:19:02.125965Z","shell.execute_reply":"2022-01-21T17:19:02.131948Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Generator that process one video through VGG16 each function call","metadata":{"id":"HGdRIG6oSooG"}},{"cell_type":"code","source":"def proces_transfer(vid_names, in_dir, labels):\n    \n    count = 0\n    \n    tam = len(vid_names)\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    while count<tam:\n        \n        video_name = vid_names[count]\n        \n        image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n        image_batch = get_frames(in_dir, video_name)\n        \n         # Note that we use 16-bit floating-points to save memory.\n        shape = (_images_per_file, transfer_values_size)\n        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n        \n        transfer_values = \\\n            image_model_transfer.predict(image_batch)\n         \n        labels1 = labels[count]\n        \n        aux = np.ones([20,2])\n        \n        labelss = labels1*aux\n        \n        yield transfer_values, labelss\n        \n        count+=1","metadata":{"id":"D2BvaY3eSpSH","execution":{"iopub.status.busy":"2022-01-21T17:19:02.134722Z","iopub.execute_input":"2022-01-21T17:19:02.134981Z","iopub.status.idle":"2022-01-21T17:19:02.145629Z","shell.execute_reply.started":"2022-01-21T17:19:02.134958Z","shell.execute_reply":"2022-01-21T17:19:02.144818Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def make_files(n_files):\n    \n    gen = proces_transfer(names_training, in_dir, labels_training)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('prueba.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","metadata":{"id":"tvU53ypSSvL0","execution":{"iopub.status.busy":"2022-01-21T17:19:02.146963Z","iopub.execute_input":"2022-01-21T17:19:02.147414Z","iopub.status.idle":"2022-01-21T17:19:02.160088Z","shell.execute_reply.started":"2022-01-21T17:19:02.147371Z","shell.execute_reply":"2022-01-21T17:19:02.159141Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def make_files_test(n_files):\n    \n    gen = proces_transfer(names_test, in_dir, labels_test)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('pruebavalidation.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","metadata":{"id":"8nK8uFExS0nX","execution":{"iopub.status.busy":"2022-01-21T17:19:02.161541Z","iopub.execute_input":"2022-01-21T17:19:02.162254Z","iopub.status.idle":"2022-01-21T17:19:02.174598Z","shell.execute_reply.started":"2022-01-21T17:19:02.162210Z","shell.execute_reply":"2022-01-21T17:19:02.173715Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Split the dataset into training set and test set\nWe are going to split the dataset into training set and testing. The training set is used to train the model and the test set to check the model accuracy.","metadata":{"id":"R9axnZ8dS64T"}},{"cell_type":"code","source":"training_set = int(len(names)*0.8)\ntest_set = int(len(names)*0.2)\n\nnames_training = names[0:training_set]\nnames_test = names[training_set:]\n\nlabels_training = labels[0:training_set]\nlabels_test = labels[training_set:]","metadata":{"id":"wjne3svdS9Y3","execution":{"iopub.status.busy":"2022-01-21T17:19:02.180266Z","iopub.execute_input":"2022-01-21T17:19:02.180511Z","iopub.status.idle":"2022-01-21T17:19:02.191962Z","shell.execute_reply.started":"2022-01-21T17:19:02.180487Z","shell.execute_reply":"2022-01-21T17:19:02.191203Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Then we are going to process all video frames through VGG16 and save the transfer values.","metadata":{"id":"xmiJi6G7TBnI"}},{"cell_type":"code","source":"make_files(training_set)","metadata":{"id":"pyO9WP-6TER4","outputId":"94209006-bfb9-470f-d418-cd2acfbdb12d","execution":{"iopub.status.busy":"2022-01-21T17:19:02.195626Z","iopub.execute_input":"2022-01-21T17:19:02.195941Z","iopub.status.idle":"2022-01-21T17:21:05.852605Z","shell.execute_reply.started":"2022-01-21T17:19:02.195911Z","shell.execute_reply":"2022-01-21T17:21:05.851779Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"make_files_test(test_set)","metadata":{"id":"0ThoefXUXPrS","outputId":"9f4cdc1e-4bfb-4ab6-81c1-05abadc1239f","execution":{"iopub.status.busy":"2022-01-21T17:21:05.855869Z","iopub.execute_input":"2022-01-21T17:21:05.856134Z","iopub.status.idle":"2022-01-21T17:21:36.328131Z","shell.execute_reply.started":"2022-01-21T17:21:05.856107Z","shell.execute_reply":"2022-01-21T17:21:36.327408Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"In order to load the saved transfer values into RAM memory we are going to use this two functions:","metadata":{"id":"RnK1JL-izzeS"}},{"cell_type":"code","source":"def process_alldata_training():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('prueba.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","metadata":{"id":"Ez0blP2z0CsF","execution":{"iopub.status.busy":"2022-01-21T17:21:36.330273Z","iopub.execute_input":"2022-01-21T17:21:36.330851Z","iopub.status.idle":"2022-01-21T17:21:36.338296Z","shell.execute_reply.started":"2022-01-21T17:21:36.330812Z","shell.execute_reply":"2022-01-21T17:21:36.337372Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def process_alldata_test():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('pruebavalidation.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","metadata":{"id":"Dne2XaQ90MGk","execution":{"iopub.status.busy":"2022-01-21T17:21:36.339883Z","iopub.execute_input":"2022-01-21T17:21:36.340597Z","iopub.status.idle":"2022-01-21T17:21:36.354090Z","shell.execute_reply.started":"2022-01-21T17:21:36.340551Z","shell.execute_reply":"2022-01-21T17:21:36.353267Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"data, target = process_alldata_training()","metadata":{"id":"phyhoYc67VW8","execution":{"iopub.status.busy":"2022-01-21T17:21:36.356694Z","iopub.execute_input":"2022-01-21T17:21:36.356960Z","iopub.status.idle":"2022-01-21T17:21:36.483954Z","shell.execute_reply.started":"2022-01-21T17:21:36.356935Z","shell.execute_reply":"2022-01-21T17:21:36.483109Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test, target_test = process_alldata_test()","metadata":{"id":"dJpXHvEg7Xhc","execution":{"iopub.status.busy":"2022-01-21T17:21:36.485406Z","iopub.execute_input":"2022-01-21T17:21:36.485750Z","iopub.status.idle":"2022-01-21T17:21:36.525771Z","shell.execute_reply.started":"2022-01-21T17:21:36.485714Z","shell.execute_reply":"2022-01-21T17:21:36.524765Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Define LSTM architecture","metadata":{"id":"BsIrL3ix4Edp"}},{"cell_type":"markdown","source":"When defining the LSTM architecture we have to take into account the dimensions of the transfer values. From each frame the VGG16 network obtains as output a vector of 4096 transfer values. From each video we are processing 20 frames so we will have 20 x 4096 values per video. The classification must be done taking into account the 20 frames of the video. If any of them detects violence, the video will be classified as violent.\n","metadata":{"id":"qFiSjMaC4Q1c"}},{"cell_type":"markdown","source":"The first input dimension of LSTM neurons is the temporal dimension, in our case it is 20. The second is the size of the features vector (transfer values).\n","metadata":{"id":"HAgSUeVu58N_"}},{"cell_type":"code","source":"chunk_size =4096\nn_chunks = 20\nrnn_size = 512\n\nmodel = Sequential()\nmodel.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))# batch_input_shape=(None, 20, 32,32 , 1)\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dense(50))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])","metadata":{"id":"XWABZ91b6f7l","execution":{"iopub.status.busy":"2022-01-21T17:21:36.528120Z","iopub.execute_input":"2022-01-21T17:21:36.528823Z","iopub.status.idle":"2022-01-21T17:21:36.892720Z","shell.execute_reply.started":"2022-01-21T17:21:36.528781Z","shell.execute_reply":"2022-01-21T17:21:36.891947Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:21:36.895051Z","iopub.execute_input":"2022-01-21T17:21:36.895415Z","iopub.status.idle":"2022-01-21T17:21:36.905973Z","shell.execute_reply.started":"2022-01-21T17:21:36.895379Z","shell.execute_reply":"2022-01-21T17:21:36.905285Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Model training\n","metadata":{"id":"bVonOPYW7F_b"}},{"cell_type":"code","source":"epoch = 500\nbatchS = 500\n\nhistory = model.fit(np.array(data[0:720]), np.array(target[0:720]), epochs=epoch,\n                    validation_data=(np.array(data[720:]), np.array(target[720:])), \n                    batch_size=batchS, verbose=2)","metadata":{"id":"iRZlW4ZV_ygS","outputId":"c223a55f-8ccd-4e5a-f5d4-dc6a1daf2d0e","execution":{"iopub.status.busy":"2022-01-21T17:21:36.907477Z","iopub.execute_input":"2022-01-21T17:21:36.907814Z","iopub.status.idle":"2022-01-21T17:23:50.444066Z","shell.execute_reply.started":"2022-01-21T17:21:36.907780Z","shell.execute_reply":"2022-01-21T17:23:50.443298Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Test the model","metadata":{"id":"BVJCgz3bA4A3"}},{"cell_type":"markdown","source":"We are going to test the model with 20 % of the total videos. This videos have not been used to train the network. ","metadata":{"id":"3MwCq0LpA59G"}},{"cell_type":"code","source":"result = model.evaluate(np.array(data_test), np.array(target_test))","metadata":{"id":"VDXFFy6zBG3X","outputId":"6ac389f9-6400-401d-dfe7-c3e90ce8a0e4","execution":{"iopub.status.busy":"2022-01-21T17:23:50.445606Z","iopub.execute_input":"2022-01-21T17:23:50.446000Z","iopub.status.idle":"2022-01-21T17:23:50.616688Z","shell.execute_reply.started":"2022-01-21T17:23:50.445961Z","shell.execute_reply":"2022-01-21T17:23:50.615727Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Print the model accuracy","metadata":{"id":"8YG-bPW4BL6U"}},{"cell_type":"code","source":"for name, value in zip(model.metrics_names, result):\n    print(name, value)","metadata":{"id":"wBV2t2Q6BOt9","outputId":"1dadcb49-b517-4cdc-9624-cb76c529b457","execution":{"iopub.status.busy":"2022-01-21T17:23:50.618251Z","iopub.execute_input":"2022-01-21T17:23:50.618630Z","iopub.status.idle":"2022-01-21T17:23:50.627244Z","shell.execute_reply.started":"2022-01-21T17:23:50.618590Z","shell.execute_reply":"2022-01-21T17:23:50.625925Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path.eps', format='eps', dpi=1000)\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path1.eps', format='eps', dpi=1000)\nplt.show()","metadata":{"id":"zO6YSbxgBZ3f","outputId":"f8a99f0c-2cf2-4228-f6ac-9083c39443fb","execution":{"iopub.status.busy":"2022-01-21T17:23:50.629406Z","iopub.execute_input":"2022-01-21T17:23:50.630369Z","iopub.status.idle":"2022-01-21T17:23:51.097006Z","shell.execute_reply.started":"2022-01-21T17:23:50.630322Z","shell.execute_reply":"2022-01-21T17:23:51.096278Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}