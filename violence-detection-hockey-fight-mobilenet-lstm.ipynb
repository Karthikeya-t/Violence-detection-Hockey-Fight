{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{"id":"UVXa7uMnOlkp"}},{"cell_type":"code","source":"%matplotlib inline\nimport cv2\nimport os\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\n# import download\nfrom random import shuffle\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Activation\nimport sys\nimport h5py","metadata":{"id":"oDfDnlliPMd-","outputId":"7582c7ed-2ba2-4164-e043-d0c8d31ba19f","execution":{"iopub.status.busy":"2022-01-21T17:07:38.419895Z","iopub.execute_input":"2022-01-21T17:07:38.420137Z","iopub.status.idle":"2022-01-21T17:07:43.099056Z","shell.execute_reply.started":"2022-01-21T17:07:38.420113Z","shell.execute_reply":"2022-01-21T17:07:43.098263Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"keras.__version__","metadata":{"id":"kFU6Qg8mPXvv","outputId":"04a2d803-e425-4335-94e8-71e91d05d6b1","execution":{"iopub.status.busy":"2022-01-21T17:07:43.100458Z","iopub.execute_input":"2022-01-21T17:07:43.100778Z","iopub.status.idle":"2022-01-21T17:07:43.109918Z","shell.execute_reply.started":"2022-01-21T17:07:43.100745Z","shell.execute_reply":"2022-01-21T17:07:43.109075Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"v6pf1l28PzIO"}},{"cell_type":"markdown","source":"We will use the function ```print_progress``` to print the amount of videos processed the datasets","metadata":{"id":"WKnyJkf8PzxE"}},{"cell_type":"code","source":"def print_progress(count, max_count):\n    # Percentage completion.\n    pct_complete = count / max_count\n\n    # Status-message. Note the \\r which means the line should\n    # overwrite itself.\n    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n\n    # Print it.\n    sys.stdout.write(msg)\n    sys.stdout.flush()","metadata":{"id":"qnafWmS7P3CG","execution":{"iopub.status.busy":"2022-01-21T17:07:43.111058Z","iopub.execute_input":"2022-01-21T17:07:43.112967Z","iopub.status.idle":"2022-01-21T17:07:43.117926Z","shell.execute_reply.started":"2022-01-21T17:07:43.112940Z","shell.execute_reply":"2022-01-21T17:07:43.117055Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n\nFirstly, we define the directory to place the video dataset","metadata":{"id":"tRf-KgkjP9Kt"}},{"cell_type":"code","source":"in_dir = \"../input/hockey-fight-vidoes/data\"","metadata":{"id":"RiRKgwBgP-NY","execution":{"iopub.status.busy":"2022-01-21T17:07:43.119264Z","iopub.execute_input":"2022-01-21T17:07:43.119697Z","iopub.status.idle":"2022-01-21T17:07:43.125546Z","shell.execute_reply.started":"2022-01-21T17:07:43.119663Z","shell.execute_reply":"2022-01-21T17:07:43.124724Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Copy some of the data-dimensions for convenience.","metadata":{"id":"UCZHrpJjRKky"}},{"cell_type":"code","source":"# Frame size  \nimg_size = 224\n\nimg_size_touple = (img_size, img_size)\n\n# Number of channels (RGB)\nnum_channels = 3\n\n# Flat frame size\nimg_size_flat = img_size * img_size * num_channels\n\n# Number of classes for classification (Violence-No Violence)\nnum_classes = 2\n\n# Number of files to train\n_num_files_train = 1\n\n# Number of frames per video\n_images_per_file = 20\n\n# Number of frames per training set\n_num_images_train = _num_files_train * _images_per_file\n\n# Video extension\nvideo_exts = \".avi\"","metadata":{"id":"SXTNEj6SRLZZ","execution":{"iopub.status.busy":"2022-01-21T17:07:43.126692Z","iopub.execute_input":"2022-01-21T17:07:43.128140Z","iopub.status.idle":"2022-01-21T17:07:43.134538Z","shell.execute_reply.started":"2022-01-21T17:07:43.128115Z","shell.execute_reply":"2022-01-21T17:07:43.133673Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Helper-function for getting video frames\nFunction used to get 20 frames from a video file and convert the frame to a suitable format for the neural net.","metadata":{"id":"Wodq7EaSRSS8"}},{"cell_type":"code","source":"def get_frames(current_dir, file_name):\n    \n    in_file = os.path.join(current_dir, file_name)\n    \n    images = []\n    \n    vidcap = cv2.VideoCapture(in_file)\n    \n    success,image = vidcap.read()\n        \n    count = 0\n\n    while count<_images_per_file:\n                \n        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n                                 interpolation=cv2.INTER_CUBIC)\n    \n        images.append(res)\n    \n        success,image = vidcap.read()\n    \n        count += 1\n        \n    resul = np.array(images)\n    \n    resul = (resul / 255.).astype(np.float16)\n        \n    return resul","metadata":{"id":"eu9c4a-3RVkO","execution":{"iopub.status.busy":"2022-01-21T17:07:43.138361Z","iopub.execute_input":"2022-01-21T17:07:43.138644Z","iopub.status.idle":"2022-01-21T17:07:43.144980Z","shell.execute_reply.started":"2022-01-21T17:07:43.138620Z","shell.execute_reply":"2022-01-21T17:07:43.144116Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Helper function to get the names of the data downloaded and label it","metadata":{"id":"tLCjYFBtRZb-"}},{"cell_type":"code","source":"def label_video_names(in_dir):\n    \n    # list containing video names\n    names = []\n    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n    labels = []\n    \n    \n    for current_dir, dir_names,file_names in os.walk(in_dir):\n        \n        for file_name in file_names:\n            \n            if file_name[0:2] == 'fi':\n                labels.append([1,0])\n                names.append(file_name)\n            elif file_name[0:2] == 'no':\n                labels.append([0,1])\n                names.append(file_name)\n                     \n            \n    c = list(zip(names,labels))\n    # Suffle the data (names and labels)\n    shuffle(c)\n    \n    names, labels = zip(*c)\n            \n    return names, labels","metadata":{"id":"Qiv5NIJjRbIA","execution":{"iopub.status.busy":"2022-01-21T17:07:43.147944Z","iopub.execute_input":"2022-01-21T17:07:43.148504Z","iopub.status.idle":"2022-01-21T17:07:43.156084Z","shell.execute_reply.started":"2022-01-21T17:07:43.148467Z","shell.execute_reply":"2022-01-21T17:07:43.155346Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Plot a video frame to see if data is correct","metadata":{"id":"t3KW2kfgReKn"}},{"cell_type":"code","source":"# First get the names and labels of the whole videos\nnames, labels = label_video_names(in_dir)","metadata":{"id":"dIsaAgcyRfIx","execution":{"iopub.status.busy":"2022-01-21T17:07:43.157466Z","iopub.execute_input":"2022-01-21T17:07:43.158032Z","iopub.status.idle":"2022-01-21T17:07:43.542536Z","shell.execute_reply.started":"2022-01-21T17:07:43.157996Z","shell.execute_reply":"2022-01-21T17:07:43.541878Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Then we are going to load 20 frames of one video, for example","metadata":{"id":"EqucOMsJRgqm"}},{"cell_type":"code","source":"names[12]","metadata":{"id":"xUfZO-0BRj0f","outputId":"e2b913dc-c39f-4d5f-a0d6-da1273692fbc","execution":{"iopub.status.busy":"2022-01-21T17:07:43.544343Z","iopub.execute_input":"2022-01-21T17:07:43.544577Z","iopub.status.idle":"2022-01-21T17:07:43.554223Z","shell.execute_reply.started":"2022-01-21T17:07:43.544555Z","shell.execute_reply":"2022-01-21T17:07:43.553212Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The video has violence, look at the name of the video, starts with 'fi'","metadata":{"id":"ORGJ2pS9RnWw"}},{"cell_type":"code","source":"frames = get_frames(in_dir, names[12])","metadata":{"id":"EqBi8z6rRoMW","execution":{"iopub.status.busy":"2022-01-21T17:07:43.555140Z","iopub.execute_input":"2022-01-21T17:07:43.555367Z","iopub.status.idle":"2022-01-21T17:07:43.713995Z","shell.execute_reply.started":"2022-01-21T17:07:43.555345Z","shell.execute_reply":"2022-01-21T17:07:43.713169Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Convert back the frames to uint8 pixel format to plot the frame","metadata":{"id":"vtgQEmI6RrmM"}},{"cell_type":"code","source":"visible_frame = (frames*255).astype('uint8')","metadata":{"id":"9ihSA_ogRsNU","execution":{"iopub.status.busy":"2022-01-21T17:07:43.716112Z","iopub.execute_input":"2022-01-21T17:07:43.716371Z","iopub.status.idle":"2022-01-21T17:07:43.766839Z","shell.execute_reply.started":"2022-01-21T17:07:43.716346Z","shell.execute_reply":"2022-01-21T17:07:43.765678Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plt.imshow(visible_frame[3])","metadata":{"id":"PM1kNhaHRvSv","outputId":"e2a6d324-cb62-42d2-dd8f-1cf1c464d168","execution":{"iopub.status.busy":"2022-01-21T17:07:43.768589Z","iopub.execute_input":"2022-01-21T17:07:43.769412Z","iopub.status.idle":"2022-01-21T17:07:43.971385Z","shell.execute_reply.started":"2022-01-21T17:07:43.769347Z","shell.execute_reply":"2022-01-21T17:07:43.970383Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plt.imshow(visible_frame[15])","metadata":{"id":"_gVWtYPvR8n2","outputId":"f9858ffa-3981-4b85-9c4d-62057bb8972c","execution":{"iopub.status.busy":"2022-01-21T17:07:43.972507Z","iopub.execute_input":"2022-01-21T17:07:43.972864Z","iopub.status.idle":"2022-01-21T17:07:44.167844Z","shell.execute_reply.started":"2022-01-21T17:07:43.972816Z","shell.execute_reply":"2022-01-21T17:07:44.167018Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Pre-Trained Model: Mobile NET","metadata":{"id":"sF1QieG5SANp"}},{"cell_type":"code","source":"image_model = MobileNet( weights='imagenet',include_top=True,input_shape=(224,224,3))","metadata":{"id":"MjRN6oE4SC81","outputId":"1a9fbd16-9741-406d-b4fd-e3a20cd34a0d","execution":{"iopub.status.busy":"2022-01-21T17:07:44.169407Z","iopub.execute_input":"2022-01-21T17:07:44.169729Z","iopub.status.idle":"2022-01-21T17:07:47.490727Z","shell.execute_reply.started":"2022-01-21T17:07:44.169696Z","shell.execute_reply":"2022-01-21T17:07:47.489990Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Let's see the model summary","metadata":{"id":"P_7z-y1mSPov"}},{"cell_type":"code","source":"image_model.summary()\n","metadata":{"id":"ud7OU0t7SQPi","outputId":"20718d3a-c1d3-4a06-d6f5-de3c465311df","execution":{"iopub.status.busy":"2022-01-21T17:07:47.493774Z","iopub.execute_input":"2022-01-21T17:07:47.494047Z","iopub.status.idle":"2022-01-21T17:07:47.523205Z","shell.execute_reply.started":"2022-01-21T17:07:47.494021Z","shell.execute_reply":"2022-01-21T17:07:47.522500Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# We will use the output of the layer prior to the final\n# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\ntransfer_layer = image_model.get_layer('reshape_2')\n\nimage_model_transfer = Model(inputs=image_model.input,\n                             outputs=transfer_layer.output)\n\ntransfer_values_size = K.int_shape(transfer_layer.output)[1]\n\n\nprint(\"The input of the MobileNet net have dimensions:\",K.int_shape(image_model.input)[1:3])\n\nprint(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)","metadata":{"id":"4YWFA-2tSdfB","outputId":"55dcb0c7-2b8a-4420-e25e-381152197a81","execution":{"iopub.status.busy":"2022-01-21T17:07:47.526616Z","iopub.execute_input":"2022-01-21T17:07:47.526877Z","iopub.status.idle":"2022-01-21T17:07:47.544541Z","shell.execute_reply.started":"2022-01-21T17:07:47.526853Z","shell.execute_reply":"2022-01-21T17:07:47.543913Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Function to process 20 video frames through VGG16 and get transfer values","metadata":{"id":"1ghum6s2Si1n"}},{"cell_type":"code","source":"def get_transfer_values(current_dir, file_name):\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n    image_batch = get_frames(current_dir, file_name)\n      \n    # Pre-allocate output-array for transfer-values.\n    # Note that we use 16-bit floating-points to save memory.\n    shape = (_images_per_file, transfer_values_size)\n    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n\n    transfer_values = \\\n            image_model_transfer.predict(image_batch)\n            \n    return transfer_values","metadata":{"id":"-AdxYAtiSlLF","execution":{"iopub.status.busy":"2022-01-21T17:07:47.548057Z","iopub.execute_input":"2022-01-21T17:07:47.548294Z","iopub.status.idle":"2022-01-21T17:07:47.555434Z","shell.execute_reply.started":"2022-01-21T17:07:47.548272Z","shell.execute_reply":"2022-01-21T17:07:47.554640Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Generator that process one video through VGG16 each function call","metadata":{"id":"HGdRIG6oSooG"}},{"cell_type":"code","source":"def proces_transfer(vid_names, in_dir, labels):\n    \n    count = 0\n    \n    tam = len(vid_names)\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    while count<tam:\n        \n        video_name = vid_names[count]\n        \n        image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n        image_batch = get_frames(in_dir, video_name)\n        \n         # Note that we use 16-bit floating-points to save memory.\n        shape = (_images_per_file, transfer_values_size)\n        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n        \n        transfer_values = \\\n            image_model_transfer.predict(image_batch)\n         \n        labels1 = labels[count]\n        \n        aux = np.ones([20,2])\n        \n        labelss = labels1*aux\n        \n        yield transfer_values, labelss\n        \n        count+=1","metadata":{"id":"D2BvaY3eSpSH","execution":{"iopub.status.busy":"2022-01-21T17:07:47.557381Z","iopub.execute_input":"2022-01-21T17:07:47.557611Z","iopub.status.idle":"2022-01-21T17:07:47.568552Z","shell.execute_reply.started":"2022-01-21T17:07:47.557589Z","shell.execute_reply":"2022-01-21T17:07:47.567827Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Functions to save transfer values from VGG16 to later use\nWe are going to define functions to get the transfer values from VGG16 with defined number of files. Then save the transfer values files used from training in one file and the ones uses for testing in another one. ","metadata":{"id":"I-Cnv0fRSswF"}},{"cell_type":"code","source":"def make_files(n_files):\n    \n    gen = proces_transfer(names_training, in_dir, labels_training)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('prueba.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","metadata":{"id":"tvU53ypSSvL0","execution":{"iopub.status.busy":"2022-01-21T17:07:47.569855Z","iopub.execute_input":"2022-01-21T17:07:47.570167Z","iopub.status.idle":"2022-01-21T17:07:47.582819Z","shell.execute_reply.started":"2022-01-21T17:07:47.570139Z","shell.execute_reply":"2022-01-21T17:07:47.581881Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def make_files_test(n_files):\n    \n    gen = proces_transfer(names_test, in_dir, labels_test)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('pruebavalidation.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","metadata":{"id":"8nK8uFExS0nX","execution":{"iopub.status.busy":"2022-01-21T17:07:47.584451Z","iopub.execute_input":"2022-01-21T17:07:47.584850Z","iopub.status.idle":"2022-01-21T17:07:47.596876Z","shell.execute_reply.started":"2022-01-21T17:07:47.584816Z","shell.execute_reply":"2022-01-21T17:07:47.596174Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Split the dataset into training set and test set\nWe are going to split the dataset into training set and testing. The training set is used to train the model and the test set to check the model accuracy.","metadata":{"id":"R9axnZ8dS64T"}},{"cell_type":"code","source":"training_set = int(len(names)*0.8)\ntest_set = int(len(names)*0.2)\n\nnames_training = names[0:training_set]\nnames_test = names[training_set:]\n\nlabels_training = labels[0:training_set]\nlabels_test = labels[training_set:]","metadata":{"id":"wjne3svdS9Y3","execution":{"iopub.status.busy":"2022-01-21T17:07:47.598468Z","iopub.execute_input":"2022-01-21T17:07:47.598896Z","iopub.status.idle":"2022-01-21T17:07:47.607286Z","shell.execute_reply.started":"2022-01-21T17:07:47.598860Z","shell.execute_reply":"2022-01-21T17:07:47.606440Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Then we are going to process all video frames through VGG16 and save the transfer values.","metadata":{"id":"xmiJi6G7TBnI"}},{"cell_type":"code","source":"make_files(training_set)","metadata":{"id":"pyO9WP-6TER4","outputId":"94209006-bfb9-470f-d418-cd2acfbdb12d","execution":{"iopub.status.busy":"2022-01-21T17:07:47.608927Z","iopub.execute_input":"2022-01-21T17:07:47.609373Z","iopub.status.idle":"2022-01-21T17:09:17.241860Z","shell.execute_reply.started":"2022-01-21T17:07:47.609336Z","shell.execute_reply":"2022-01-21T17:09:17.240896Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"make_files_test(test_set)","metadata":{"id":"0ThoefXUXPrS","outputId":"9f4cdc1e-4bfb-4ab6-81c1-05abadc1239f","execution":{"iopub.status.busy":"2022-01-21T17:09:17.243200Z","iopub.execute_input":"2022-01-21T17:09:17.243542Z","iopub.status.idle":"2022-01-21T17:09:39.096490Z","shell.execute_reply.started":"2022-01-21T17:09:17.243506Z","shell.execute_reply":"2022-01-21T17:09:39.095817Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Load the cached transfer values into memory\nWe have already saved all the videos transfer values into disk. But we have to load those transfer values into memory in order to train the LSTM net. One question would be: why not process transfer values and load them into RAM memory? Yes is a more eficient way to train the second net. But if you have to train the LSTM in different ways in order to see which way gets the best accuracy, if you didn't save the transfer values into disk you would have to process the whole videos each training. It's very time consuming processing the videos through VGG16 net. \n","metadata":{"id":"0s2imcRixMAr"}},{"cell_type":"markdown","source":"In order to load the saved transfer values into RAM memory we are going to use this two functions:","metadata":{"id":"RnK1JL-izzeS"}},{"cell_type":"code","source":"def process_alldata_training():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('prueba.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","metadata":{"id":"Ez0blP2z0CsF","execution":{"iopub.status.busy":"2022-01-21T17:09:39.097740Z","iopub.execute_input":"2022-01-21T17:09:39.098067Z","iopub.status.idle":"2022-01-21T17:09:39.105971Z","shell.execute_reply.started":"2022-01-21T17:09:39.098039Z","shell.execute_reply":"2022-01-21T17:09:39.104099Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def process_alldata_test():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('pruebavalidation.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","metadata":{"id":"Dne2XaQ90MGk","execution":{"iopub.status.busy":"2022-01-21T17:09:39.107243Z","iopub.execute_input":"2022-01-21T17:09:39.107607Z","iopub.status.idle":"2022-01-21T17:09:39.116957Z","shell.execute_reply.started":"2022-01-21T17:09:39.107572Z","shell.execute_reply":"2022-01-21T17:09:39.116276Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"data, target = process_alldata_training()","metadata":{"id":"phyhoYc67VW8","execution":{"iopub.status.busy":"2022-01-21T17:09:39.118224Z","iopub.execute_input":"2022-01-21T17:09:39.118995Z","iopub.status.idle":"2022-01-21T17:09:39.154541Z","shell.execute_reply.started":"2022-01-21T17:09:39.118958Z","shell.execute_reply":"2022-01-21T17:09:39.153751Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"data_test, target_test = process_alldata_test()","metadata":{"id":"dJpXHvEg7Xhc","execution":{"iopub.status.busy":"2022-01-21T17:09:39.156074Z","iopub.execute_input":"2022-01-21T17:09:39.156497Z","iopub.status.idle":"2022-01-21T17:09:39.169371Z","shell.execute_reply.started":"2022-01-21T17:09:39.156460Z","shell.execute_reply":"2022-01-21T17:09:39.168623Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Define LSTM architecture","metadata":{"id":"BsIrL3ix4Edp"}},{"cell_type":"markdown","source":"When defining the LSTM architecture we have to take into account the dimensions of the transfer values. From each frame the VGG16 network obtains as output a vector of 4096 transfer values. From each video we are processing 20 frames so we will have 20 x 4096 values per video. The classification must be done taking into account the 20 frames of the video. If any of them detects violence, the video will be classified as violent.\n","metadata":{"id":"qFiSjMaC4Q1c"}},{"cell_type":"markdown","source":"The first input dimension of LSTM neurons is the temporal dimension, in our case it is 20. The second is the size of the features vector (transfer values).\n","metadata":{"id":"HAgSUeVu58N_"}},{"cell_type":"code","source":"chunk_size = 1000\nn_chunks = 20\nrnn_size = 512\n\nmodel = Sequential()\nmodel.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dense(50))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])","metadata":{"id":"XWABZ91b6f7l","execution":{"iopub.status.busy":"2022-01-21T17:09:39.170885Z","iopub.execute_input":"2022-01-21T17:09:39.171303Z","iopub.status.idle":"2022-01-21T17:09:39.501495Z","shell.execute_reply.started":"2022-01-21T17:09:39.171267Z","shell.execute_reply":"2022-01-21T17:09:39.500782Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Model training\n","metadata":{"id":"bVonOPYW7F_b"}},{"cell_type":"code","source":"epoch = 100\nbatchS = 500\n\nhistory = model.fit(np.array(data[0:720]), np.array(target[0:720]), epochs=epoch,\n                    validation_data=(np.array(data[720:]), np.array(target[720:])), \n                    batch_size=batchS, verbose=2)","metadata":{"id":"iRZlW4ZV_ygS","outputId":"c223a55f-8ccd-4e5a-f5d4-dc6a1daf2d0e","execution":{"iopub.status.busy":"2022-01-21T17:09:39.502640Z","iopub.execute_input":"2022-01-21T17:09:39.502977Z","iopub.status.idle":"2022-01-21T17:09:52.084365Z","shell.execute_reply.started":"2022-01-21T17:09:39.502944Z","shell.execute_reply":"2022-01-21T17:09:52.083666Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Test the model","metadata":{"id":"BVJCgz3bA4A3"}},{"cell_type":"markdown","source":"We are going to test the model with 20 % of the total videos. This videos have not been used to train the network. ","metadata":{"id":"3MwCq0LpA59G"}},{"cell_type":"code","source":"result = model.evaluate(np.array(data_test), np.array(target_test))","metadata":{"id":"VDXFFy6zBG3X","outputId":"6ac389f9-6400-401d-dfe7-c3e90ce8a0e4","execution":{"iopub.status.busy":"2022-01-21T17:10:14.099808Z","iopub.execute_input":"2022-01-21T17:10:14.100144Z","iopub.status.idle":"2022-01-21T17:10:14.178578Z","shell.execute_reply.started":"2022-01-21T17:10:14.100116Z","shell.execute_reply":"2022-01-21T17:10:14.177881Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Print the model accuracy","metadata":{"id":"8YG-bPW4BL6U"}},{"cell_type":"code","source":"for name, value in zip(model.metrics_names, result):\n    print(name, value)","metadata":{"id":"wBV2t2Q6BOt9","outputId":"1dadcb49-b517-4cdc-9624-cb76c529b457","execution":{"iopub.status.busy":"2022-01-21T17:10:18.794288Z","iopub.execute_input":"2022-01-21T17:10:18.794604Z","iopub.status.idle":"2022-01-21T17:10:18.801879Z","shell.execute_reply.started":"2022-01-21T17:10:18.794573Z","shell.execute_reply":"2022-01-21T17:10:18.800863Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path.eps', format='eps', dpi=1000)\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path1.eps', format='eps', dpi=1000)\nplt.show()","metadata":{"id":"zO6YSbxgBZ3f","outputId":"f8a99f0c-2cf2-4228-f6ac-9083c39443fb","execution":{"iopub.status.busy":"2022-01-21T17:10:22.051830Z","iopub.execute_input":"2022-01-21T17:10:22.052154Z","iopub.status.idle":"2022-01-21T17:10:22.451054Z","shell.execute_reply.started":"2022-01-21T17:10:22.052124Z","shell.execute_reply":"2022-01-21T17:10:22.450220Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}